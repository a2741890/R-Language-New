---
title: "509Project"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Task1:Data Mining

(a)Use a 70-30 split to create your training and test data.
```{r}
library(party)
#read the data file yeast.data
yeast_table <- read.table("yeast.data")
#give it column name
colnames(yeast_table) <- c("Sequence.Name","mcg","gvh","alm","mit","erl","pox","vac","nuc","Localisation.Site")
yeast_table <- data.frame(yeast_table)
#feed random seed
set.seed(1234)
#Split the data into 70% training data, 30% test data.
ind <- sample(2, nrow(yeast_table), replace = TRUE, prob = c(0.7,0.3))
train_data <- yeast_table[ind==1,]
test_data <- yeast_table[ind==2,]
```
(b)Use your training data to train a model.
```{r}
#Set the target variable and the independent variables
formula <- Localisation.Site ~  mcg + gvh + alm + mit + erl + pox + vac + nuc
#train the decision tree by the training data
yeast_ctree <- ctree(formula, data= train_data)
```
(c)Use your model to predict previously unseen data using the test data.
```{r}
#create a confusion matrix
confusionM <- table(predict(yeast_ctree, newdata=test_data), test_data$Localisation.Site)
confusionM
```
(d)
```{r}
library(caret)
#create a confusion matrix by package "caret"
a <- confusionMatrix(predict(yeast_ctree, newdata=test_data),test_data$Localisation.Site)
a
```
Summary: 
According to the outcome of the confusion matrix, the model overall accuracy((True Positive + True Negative)/ all dataset) is 59.26%. And we have 95% confidence in clarifying that the real accuracy is located between 54.46% and  63.39%. Since the p-value < 2.2e-16, obviously less than 0.05(alpha), so that we accept the alternative hypothesis - accuracy is better than no information rate(Acc>NIR).

Task2:
(a)Produce a visualization of your classification model and how it makes decisions.
```{r fig.width=90, fig.height=40}
library(party)
#print out the decision tree
plot(yeast_ctree,terminal_panel=node_barplot(yeast_ctree, fill = "red", beside = TRUE,
  ymax = NULL, ylines = NULL, widths = 0.1, gap = 0.01, 
  reverse = NULL, id = TRUE))  
```
I already do my best to display the whole diagram but still can not show the details clearly. Please right click the mouse to open and zoon in to see the detail.

Summary:
Decision making example:
1. alm <= 0.43 -> mcg <= 0.68 -> mit <= 0.48 -> alm <= 0.37 -> There are 74 datasets in node 5, and more than 80% of localisation sites belong to ME3. ME2, MIT, NUC and VAC only have very little percentage.

2. alm <= 0.43 -> mcg <= 0.68 -> mit <= 0.48 -> alm > 0.37 -> There are 70 datasets in node 6, and more than 60% of localisation sites belong to ME3. Around 20% belongs to NUC. ME2, MIT, POX and VAC only have very little percentage.

3. alm <= 0.43 -> mcg <= 0.68 -> mit > 0.48  -> There are 7 datasets in node 7, and more than 60% of localisation sites belong to MIT. And the localisation sites including ME2, ME3 and NUC all have a percentage of 20.
.
.
.
Similarly, the other decision path can be yielded with the same logic.

Explanation:
Therefore, according to the examples mentioned above, we can easily understand that if the alm of the yeast is less or equal than 0.43, its mcg is equal or less than 0.68, its mitis less or equal than 0.48 and its alm again is less or equal than 0.37 so that based on our decision tree, we know there are 74 datasets out of all our datasets in our testing data matched with these conditions. In these 74 datasets, we have more than 80% yeast that their localisation site are ME3. However, the localisation sites of the yeast which belong to ME2, MIT, NUC and VAC only have very little proportion.

(b)Produce a visualization of your confusion matrix as a heatmap.
```{r}
#change a confusion matrix class into a data frame.
confusionM <- data.frame(confusionM)
#find out the maxium and minium value in the frequency column of confusionM
top <- max(confusionM$Freq)
bottom <- min(confusionM$Freq)
#normalize the original values and set the new range [0,1] by using Min-max normalization method
temp <- 0
for(i in 1:length(confusionM$Freq)){
  temp[i]<- (confusionM$Freq[i]-bottom)/(top-bottom)
}
#add the normalized values as a new column into the confusionM
confusionM <- data.frame(cbind(confusionM, temp))
confusionM$temp <- as.double(confusionM$temp)
library(ggplot2)
#print out the heatmap of the confusionM
ggplot(data =  confusionM, mapping = aes(x = confusionM$Var1, y = confusionM$Var2)) +
  geom_tile(aes(fill = confusionM$temp), colour = "white") +
  geom_text(aes(label = sprintf("%1.5f", confusionM$temp)), vjust = 1, size = 3) +scale_fill_gradient2('temp', limits=c(0, 1), breaks = c(0, 0.25,0.5,0.75, 1),  low = "green",mid = "yellow", high = "red", midpoint = 0.5)+
   theme(legend.title=element_blank()) +labs(title="Confusion Matrix Heatmap", y="Prediction", x="Reference")
```

Task3: Data Analysis
(a)Write two or three paragraphs to explain the topics on the following:

1.Explain the result reported in the confusion matrix, with respect to true and false positives.

According to the definition of confusion matrix, the TP(true positives) means the classifier predict the yeast as one of the localisation site, and the yeast actually belongs to that localisation site. As for FP(false positives), it means the classifier predict the yeast as one of the localisation, but the yeast don't actually match with the result of the prediction.
In terms of my confusion matrix, the TP and FP for each localisation site are listed below:(Total number of datasets in test date is 432 )
TP(the counts on the diagonal of matrix):
CYT=69, ERL=0, EXC=9, ME1=7, ME2=7, ME3=42, MIT=52, NUC=67, POX=3, VAC=0
FP(the number of non-zero values on each column - reference):
CYT=47, ERL=0, EXC=9, ME1=4, ME2=4, ME3=20, MIT=21, NUC=71, POX=0, VAC=0

2.If you think classifier you have created are acceptable in terms of their effectiveness.

Yes, it is acceptable. The overall accuracy is 59.26%, and we have 95% confidence to clarify the actual accuracy is located between 54.46% and 63.93%. The p-value is far less than the general set alpha=0.05 so that we accept the accuracy rate is bigger than the NIR(No information rate). In terms of the Kappa value, it is in relation to how well the classifier performed as compared to how well it would have performed simply by chance. Simply to say, a model with a high Kappa value means there is a big difference between the accuracy and the NIR. For this model, the Kappa value is 0.481. And as I check Wiki, citing from  Landis, J.R.; Koch, G.G. (1977). "The measurement of observer agreement for categorical data". Biometrics.33 (1): 159¡V174, they  considers the Kappa value 0-0.20 as slight, 0.21-0.40 as fair, 0.41-0.60 as moderate, 0.61-0.80 as substantial, and 0.81-1 as almost perfect. So my Kappa value in this model  is in the range of 0.41-0.60 which means this classifier is quite acceptable.


3.Why you think the model has made the predictions it did: reflect on this especially with respect to the distributions of class variables.

For the class CYT, EXC, ME1, ME2, ME3, MIT, NUC, and POX, they all have a relatively high sensitivity and specificity so that their PPV(positive predictive value) and NPV(negative predictive value) are high as well. As for ERL and VAC, both of them have zero sensitivity but have 100% specificity. Overall, for all the classes, they all have accuracy for at least 50% so that I would say this model has basically made the predictions it did.   

(b)Remove some independent values to make decision tree simpler.

1.Provide a method to remove a few variables from column 2 to column 9.

In here, I used a common method called Information gain to select the feature based on information theory. By using the package "FSelector", I applied the method inside called information.gain(), and input the target variable and independent varibles to this method. After this, I got the weights of each attribute based on their correlation with continuous class attribute. As looking into the output, I found that the features including erl with importance 0.01842510, pox with importance 0.03151217, vac with importance 0.00 and nuc with importance 0.08241855, are all under 0.1 regarding to their importance. Therefore, my conclusion is that they have less weights with others class so that I decided to remove these 4 features to make my decision tree more efficient and simplier. 
```{r}
library(FSelector)
#using the method information.gain() in FSelector package, we can get the weights of each attribute
weight <- information.gain(formula, yeast_table)
weight
#pick the more important features and make a new formula for the decision tree
new_formula <- Localisation.Site ~ mcg + gvh + alm + mit   
```

2.Use a 70-30 split to create a new training model by using your selected independent variables and the target variable "localisation sites".
```{r fig.width=90, fig.height= 40, fig.asp=0.5}
#Feed a random seed.
set.seed(1234)
#Split the data into 70% training data, 30% test data.
ind <- sample(2, nrow(yeast_table), replace = TRUE, prob = c(0.7,0.3))
#Choosing the data from the yeast.data with a ratio 70% training: 30% testing
new_train_data <- yeast_table[ind==1,]
new_test_data <- yeast_table[ind==2,]
#Using the new formula which only contains my selected independent variables and the new training data to create a new decision tree.
new_yeast_ctree <- ctree(new_formula, data= new_train_data)
#Create the confusion matrix
new_confusionM <- table(predict(new_yeast_ctree, newdata=new_test_data), new_test_data$Localisation.Site)
new_confusionM
#create the confusion matrix by the package "caret"
confusionMatrix(predict(new_yeast_ctree, newdata=new_test_data),new_test_data$Localisation.Site)

#output my new decision tree
plot(new_yeast_ctree)
```
I already do my best to display the whole diagram but still can not show the details clearly. Please right click the mouse to open and zoon in to see the detail.

3.Discuss your experimental results(i.e., confusion matrix) against the results in Task2(b).
```{r}
#the heatmap for Task2(b)
b <-ggplot(data =  confusionM, mapping = aes(x = confusionM$Var1, y = confusionM$Var2)) +
  geom_tile(aes(fill = confusionM$temp), colour = "white") +
  geom_text(aes(label = sprintf("%1.5f", confusionM$temp)), vjust = 1, size = 3) +scale_fill_gradient2('temp', limits=c(0, 1), breaks = c(0, 0.25,0.5,0.75, 1),  low = "green",mid = "yellow", high = "red", midpoint = 0.5)+
   theme(legend.title=element_blank()) +labs(title="Confusion Matrix Heatmap", y="Prediction", x="Reference")
b
#the heatmap for Task3
new_confusionM <- data.frame(new_confusionM)
new_top <- max(new_confusionM$Freq)
new_bottom <- min(new_confusionM$Freq)
new_temp <- 0
for(i in 1:length(new_confusionM$Freq)){
new_temp[i]<- (new_confusionM$Freq[i]-new_bottom)/(new_top-new_bottom)
}

new_confusionM <- cbind(new_confusionM, new_temp)
new_confusionM$new_temp <- as.double(new_confusionM$new_temp)
c <- ggplot(data =  new_confusionM, mapping = aes(x = new_confusionM$Var1, y = new_confusionM$Var2)) +
  geom_tile(aes(fill = new_confusionM$new_temp), colour = "white") +
  geom_text(aes(label = sprintf("%1.5f", new_confusionM$new_temp)), vjust = 1, size = 3) +scale_fill_gradient2('new_temp', limits=c(0, 1), breaks = c(0, 0.25,0.5,0.75, 1),  low = "green",mid = "yellow", high = "red", midpoint = 0.5)+
   theme(legend.title=element_blank()) +labs(title="NEW Confusion Matrix Heatmap", y="Prediction", x="Reference")
c
```

Discussion:
First we compare the confusion matrix, before the feature selection, the accuracy is 0.5926 and after feature selection is 0.5671. We can see the decrease in the accuracy. Therefore, we can see the difference occuring on the heatmap, which the high values in the original one distribute on the diagonal more than the new one. The pattern of the distribution in the new heatmap seems to be more average. 

I reckon the reason for this change is because there are less independent variables for the classifier to use as it is making the decision. However, I achieved the goal that to make the decision tree simpler by removing unimportant features. In another word, the old decision tree has 18 terminal nodes while the new one only has 17 terminal nodes. 
 

